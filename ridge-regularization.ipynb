{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:55:01.091535Z","iopub.execute_input":"2025-08-27T12:55:01.092837Z","iopub.status.idle":"2025-08-27T12:55:02.872176Z","shell.execute_reply.started":"2025-08-27T12:55:01.092787Z","shell.execute_reply":"2025-08-27T12:55:02.870777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, SGDRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T13:04:49.823149Z","iopub.execute_input":"2025-08-27T13:04:49.823506Z","iopub.status.idle":"2025-08-27T13:04:49.829453Z","shell.execute_reply.started":"2025-08-27T13:04:49.823482Z","shell.execute_reply":"2025-08-27T13:04:49.828246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, y = load_diabetes(return_X_y=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:55:05.269517Z","iopub.execute_input":"2025-08-27T12:55:05.270000Z","iopub.status.idle":"2025-08-27T12:55:05.291732Z","shell.execute_reply.started":"2025-08-27T12:55:05.269969Z","shell.execute_reply":"2025-08-27T12:55:05.290568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:55:05.293920Z","iopub.execute_input":"2025-08-27T12:55:05.294981Z","iopub.status.idle":"2025-08-27T12:55:05.303481Z","shell.execute_reply.started":"2025-08-27T12:55:05.294941Z","shell.execute_reply":"2025-08-27T12:55:05.302246Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# With Simple Linear Regression","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:55:05.304386Z","iopub.execute_input":"2025-08-27T12:55:05.304741Z","iopub.status.idle":"2025-08-27T12:55:05.334342Z","shell.execute_reply.started":"2025-08-27T12:55:05.304710Z","shell.execute_reply":"2025-08-27T12:55:05.333108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr = LinearRegression()\n\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\n\nr2score = r2_score(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred)) \nprint(r2score)\nprint(rmse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:55:05.335438Z","iopub.execute_input":"2025-08-27T12:55:05.335809Z","iopub.status.idle":"2025-08-27T12:55:05.397375Z","shell.execute_reply.started":"2025-08-27T12:55:05.335774Z","shell.execute_reply":"2025-08-27T12:55:05.395967Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# With Ridge Regularization","metadata":{}},{"cell_type":"code","source":"# NOTE WE CAN ALSO USE SOLVER DIRECTLY IN RIDGE TO IMPLEMENT GRADIENT DESCENT HERE\n\nridge_alpha = 70\ndegree = 2\nwith_bias = False\n\nmodel = Pipeline([\n    ('poly_feats', PolynomialFeatures(degree = degree, include_bias = with_bias )),\n    ('standard_scaling', StandardScaler()),\n    ('ridge', Ridge(alpha=ridge_alpha))\n    #('ridge', RidgeCV(alphas=[0.1, 1, 10, 100]))\n])\n\nmodel.fit(X_train, y_train)\n\nr_y_pred = model.predict(X_test)\n\nr_r2score = r2_score(y_test, r_y_pred)\nr_rmse = np.sqrt(mean_squared_error(y_test, r_y_pred))\n\nprint(f\"r_r2score : {r_r2score} \")\nprint(f\"r_rmse : {r_rmse} \")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:56:10.130208Z","iopub.execute_input":"2025-08-27T12:56:10.130602Z","iopub.status.idle":"2025-08-27T12:56:10.144120Z","shell.execute_reply.started":"2025-08-27T12:56:10.130572Z","shell.execute_reply":"2025-08-27T12:56:10.143032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:55:09.619739Z","iopub.execute_input":"2025-08-27T12:55:09.620127Z","iopub.status.idle":"2025-08-27T12:55:09.628341Z","shell.execute_reply.started":"2025-08-27T12:55:09.620090Z","shell.execute_reply":"2025-08-27T12:55:09.627181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(y_test.values if hasattr(y_test, \"values\") else y_test, label=\"Actual\", marker='o')\nplt.plot(r_y_pred, label=\"Predicted\", marker='x')\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Target Value\")\nplt.title(\"Actual vs Predicted Values (Ridge Regression)\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:55:09.629627Z","iopub.execute_input":"2025-08-27T12:55:09.630038Z","iopub.status.idle":"2025-08-27T12:55:09.991490Z","shell.execute_reply.started":"2025-08-27T12:55:09.630001Z","shell.execute_reply":"2025-08-27T12:55:09.990313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(7,7))\nplt.scatter(y_test, r_y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)  # perfect prediction line\nplt.xlabel(\"Actual Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Ridge Regression Predictions vs Actual\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:56:14.082018Z","iopub.execute_input":"2025-08-27T12:56:14.082373Z","iopub.status.idle":"2025-08-27T12:56:14.286638Z","shell.execute_reply.started":"2025-08-27T12:56:14.082345Z","shell.execute_reply":"2025-08-27T12:56:14.285257Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Polynomial Features With Ridge Regularization inside SGD Rgressor for large dataset","metadata":{}},{"cell_type":"code","source":"# we have to play with combination of ridge_alpha & degree to create best hyperparameters\n\n# NOTE WE CAN ALSO USE SOLVER DIRECTLY IN RIDGE TO IMPLEMENT GRADIENT DESCENT THERE\n\nridge_alpha = 0.2\ndegree = 2\nwith_bias = True\n\nmodel = Pipeline([\n    ('poly_feats', PolynomialFeatures(degree = degree, include_bias = with_bias )),\n    ('standard_scaling', StandardScaler()),\n    (\"sgd\", SGDRegressor(\n        penalty=\"l2\",       # Ridge-like regularization\n        alpha = ridge_alpha,        # Regularization strength\n        max_iter=1000,      # more iterations for convergence\n        tol=1e-3, # tolerance for stopping criterion\n        random_state=42\n    ))\n    #('ridge', RidgeCV(alphas=[0.1, 1, 10, 100]))\n])\n\nmodel.fit(X_train, y_train)\n\nr_y_pred = model.predict(X_test)\n\nr_r2score = r2_score(y_test, r_y_pred)\nr_rmse = np.sqrt(mean_squared_error(y_test, r_y_pred))\n\nprint(f\"r_r2score : {r_r2score} \")\nprint(f\"r_rmse : {r_rmse} \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T13:09:11.072110Z","iopub.execute_input":"2025-08-27T13:09:11.072464Z","iopub.status.idle":"2025-08-27T13:09:11.086724Z","shell.execute_reply.started":"2025-08-27T13:09:11.072423Z","shell.execute_reply":"2025-08-27T13:09:11.085618Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SGD VS RIDGE\n\n**SGDRegressor with Polynomial Features**\n\n1. SGDRegressor uses stochastic gradient descent to optimize a linear model, which can include polynomial features.\n2. It is efficient for large-scale datasets with many samples.\n3. Regularization can be applied (L2, L1, or Elastic Net), but hyperparameter tuning (e.g., learning rate, number of iterations) is critical.\n4. SGD may converge slower or be more sensitive to parameter settings and scaling of features.\n5. Often used for online or incremental learning scenarios or when computation speed and memory efficiency are priorities.\n\n**Ridge Regression with Polynomial Features**\n\n1. Ridge regression explicitly adds L2 regularization to the least squares cost function to prevent overfitting on polynomial features.\n2. It usually provides more stable and interpretable solutions for polynomial regression tasks.\n3. Ridge tends to achieve better and more consistent generalization performance because the solution minimizes a convex problem in a closed form or via efficient solvers.\n4. Better suited for small to medium datasets where full batch optimization is feasible.\n5. Often preferred when control over regularization strength and model interpretability is important.","metadata":{}}]}